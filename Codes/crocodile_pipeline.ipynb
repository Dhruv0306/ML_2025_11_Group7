{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af24f05c",
   "metadata": {},
   "source": [
    "# Crocodile Identification Pipeline\n",
    "\n",
    "This notebook implements a complete pipeline for automated biometric identification of Mugger Crocodiles using UAV images. The pipeline includes:\n",
    "- Dataset cleaning and balancing\n",
    "- Feature extraction using SIFT, HOG, LBP, and ORB\n",
    "- Dimensionality reduction using PCA\n",
    "- Multiple model training and evaluation\n",
    "- Visualization of results and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c2540",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80a2df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Memory\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "print(\"All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd7b2e",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fda2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"\n",
    "    Create a directory if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        path (str): Directory path to create\n",
    "    \"\"\"\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_voc_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Parse Pascal VOC XML file to get bounding box coordinates\n",
    "    \n",
    "    Args:\n",
    "        xml_path (str): Path to XML file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (xmin, ymin, xmax, ymax) coordinates\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Get bounding box coordinates\n",
    "    obj = root.find('object')\n",
    "    bbox = obj.find('bndbox')\n",
    "    xmin = int(bbox.find('xmin').text)\n",
    "    ymin = int(bbox.find('ymin').text)\n",
    "    xmax = int(bbox.find('xmax').text)\n",
    "    ymax = int(bbox.find('ymax').text)\n",
    "    \n",
    "    return (xmin, ymin, xmax, ymax)\n",
    "\n",
    "def crop_image(image_path, bbox=None, output_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Crop image using bounding box or center crop\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to input image\n",
    "        bbox (tuple, optional): (xmin, ymin, xmax, ymax) coordinates\n",
    "        output_size (tuple): Desired output size (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Cropped image\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    if bbox:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        cropped = img[ymin:ymax, xmin:xmax]\n",
    "    else:\n",
    "        # Center crop\n",
    "        h, w = img.shape[:2]\n",
    "        center_x, center_y = w // 2, h // 2\n",
    "        xmin = max(0, center_x - output_size[0] // 2)\n",
    "        ymin = max(0, center_y - output_size[1] // 2)\n",
    "        xmax = min(w, xmin + output_size[0])\n",
    "        ymax = min(h, ymin + output_size[1])\n",
    "        cropped = img[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    # Resize to standard size\n",
    "    cropped = cv2.resize(cropped, output_size)\n",
    "    return cropped\n",
    "\n",
    "def extract_croc_id_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract crocodile ID from filename\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Input filename (e.g., 'Croc1_1.jpg')\n",
    "        \n",
    "    Returns:\n",
    "        str: Crocodile ID (e.g., 'Croc1')\n",
    "    \"\"\"\n",
    "    return filename.split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14cb10",
   "metadata": {},
   "source": [
    "## 3. Dataset Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b097bba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Validate XML file for proper bounding box information.\n",
    "    \n",
    "    Args:\n",
    "        xml_path (str): Path to the XML file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if XML is valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Check if there are any object tags\n",
    "        objects = root.findall('.//object')\n",
    "        if not objects:\n",
    "            return False\n",
    "        \n",
    "        # Check if each object has valid bounding box\n",
    "        for obj in objects:\n",
    "            bndbox = obj.find('bndbox')\n",
    "            if bndbox is None:\n",
    "                return False\n",
    "            \n",
    "            # Check if all required coordinates are present and valid\n",
    "            for coord in ['xmin', 'ymin', 'xmax', 'ymax']:\n",
    "                if bndbox.find(coord) is None:\n",
    "                    return False\n",
    "                try:\n",
    "                    float(bndbox.find(coord).text)\n",
    "                except (ValueError, TypeError):\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating XML {xml_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_min_folder_size(directory):\n",
    "    \"\"\"\n",
    "    Get the minimum number of valid image-XML pairs across all crocodile folders.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the training dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        int: Minimum number of valid pairs\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 1] Analyzing folders to find minimum number of valid pairs...\")\n",
    "    min_size = float('inf')\n",
    "    folder_counts = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if \"removed\" in root:\n",
    "            continue\n",
    "        \n",
    "        jpg_files = set(f for f in files if f.endswith('.jpg'))\n",
    "        xml_files = set(f.replace('.jpg', '.xml') for f in jpg_files)\n",
    "        \n",
    "        # Count only valid pairs\n",
    "        valid_pairs = sum(1 for xml in xml_files if xml in files and validate_xml(os.path.join(root, xml)))\n",
    "        \n",
    "        if valid_pairs > 0:\n",
    "            min_size = min(min_size, valid_pairs)\n",
    "            folder_name = os.path.basename(root)\n",
    "            folder_counts[folder_name] = valid_pairs\n",
    "    \n",
    "    # Print folder statistics\n",
    "    print(f\"\\nFound {len(folder_counts)} folders with valid image-XML pairs:\")\n",
    "    for folder, count in sorted(folder_counts.items()):\n",
    "        print(f\"  - {folder}: {count} valid pairs\")\n",
    "    \n",
    "    return min_size if min_size != float('inf') else 0\n",
    "\n",
    "def clean_and_balance_dataset(directory):\n",
    "    \"\"\"\n",
    "    Clean the dataset by moving invalid files to a removed directory and balance the number of images.\n",
    "    Recursively checks all subfolders within the given directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the root training dataset directory\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 2] Setting up removed directory structure...\")\n",
    "    \n",
    "    # Use the existing removed directory\n",
    "    removed_dir = os.path.join(os.path.dirname(os.path.dirname(directory)), \"removed\", \"Training\")\n",
    "    os.makedirs(removed_dir, exist_ok=True)\n",
    "    print(f\"Using existing removed directory: {removed_dir}\")\n",
    "    \n",
    "    # First, get the minimum folder size\n",
    "    min_size = get_min_folder_size(directory)\n",
    "    print(f\"\\n[STEP 3] Minimum number of valid pairs across all folders: {min_size}\")\n",
    "    \n",
    "    total_moved_jpg = 0\n",
    "    total_moved_xml = 0\n",
    "    \n",
    "    print(\"\\n[STEP 4] Processing each folder to clean and balance the dataset...\")\n",
    "    \n",
    "    # Walk through all subdirectories\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Skip the removed directory\n",
    "        if \"removed\" in root:\n",
    "            continue\n",
    "        \n",
    "        # Get the CrocID from the current directory path\n",
    "        croc_id = os.path.basename(root)\n",
    "        print(f\"\\n  Processing folder: {croc_id}\")\n",
    "        \n",
    "        # Get all jpg and xml files in current directory\n",
    "        jpg_files = set(glob.glob(os.path.join(root, \"*.jpg\")))\n",
    "        xml_files = set(glob.glob(os.path.join(root, \"*.xml\")))\n",
    "        \n",
    "        print(f\"    Found {len(jpg_files)} JPG files and {len(xml_files)} XML files\")\n",
    "        \n",
    "        # Convert to sets of filenames without extensions\n",
    "        jpg_bases = {os.path.splitext(os.path.basename(f))[0] for f in jpg_files}\n",
    "        xml_bases = {os.path.splitext(os.path.basename(f))[0] for f in xml_files}\n",
    "        \n",
    "        # Create corresponding removed directory\n",
    "        removed_croc_dir = os.path.join(removed_dir, croc_id)\n",
    "        os.makedirs(removed_croc_dir, exist_ok=True)\n",
    "        \n",
    "        # Find orphaned JPG files (no corresponding XML)\n",
    "        orphaned_jpg = jpg_bases - xml_bases\n",
    "        \n",
    "        # Move orphaned JPG files\n",
    "        if orphaned_jpg:\n",
    "            print(f\"    Found {len(orphaned_jpg)} orphaned JPG files (no corresponding XML)\")\n",
    "            for base in orphaned_jpg:\n",
    "                src_path = os.path.join(root, f\"{base}.jpg\")\n",
    "                dst_path = os.path.join(removed_croc_dir, f\"{base}.jpg\")\n",
    "                try:\n",
    "                    shutil.move(src_path, dst_path)\n",
    "                    print(f\"      Moved orphaned image: {base}.jpg\")\n",
    "                    total_moved_jpg += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error moving {base}.jpg: {str(e)}\")\n",
    "        \n",
    "        # Get valid image-XML pairs\n",
    "        valid_pairs = []\n",
    "        invalid_pairs = []\n",
    "        \n",
    "        for base in xml_bases:\n",
    "            xml_path = os.path.join(root, f\"{base}.xml\")\n",
    "            jpg_path = os.path.join(root, f\"{base}.jpg\")\n",
    "            \n",
    "            # Check if both files exist and XML is valid\n",
    "            if os.path.exists(jpg_path) and validate_xml(xml_path):\n",
    "                valid_pairs.append((base, xml_path, jpg_path))\n",
    "            else:\n",
    "                invalid_pairs.append((base, xml_path, jpg_path))\n",
    "        \n",
    "        # Move invalid pairs\n",
    "        if invalid_pairs:\n",
    "            print(f\"    Found {len(invalid_pairs)} invalid pairs (missing or invalid XML)\")\n",
    "            for base, xml_path, jpg_path in invalid_pairs:\n",
    "                try:\n",
    "                    if os.path.exists(xml_path):\n",
    "                        xml_dst = os.path.join(removed_croc_dir, f\"{base}.xml\")\n",
    "                        shutil.move(xml_path, xml_dst)\n",
    "                        print(f\"      Moved invalid XML: {base}.xml\")\n",
    "                        total_moved_xml += 1\n",
    "                    \n",
    "                    if os.path.exists(jpg_path):\n",
    "                        jpg_dst = os.path.join(removed_croc_dir, f\"{base}.jpg\")\n",
    "                        shutil.move(jpg_path, jpg_dst)\n",
    "                        print(f\"      Moved corresponding image: {base}.jpg\")\n",
    "                        total_moved_jpg += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error moving files for {base}: {str(e)}\")\n",
    "        \n",
    "        print(f\"    Found {len(valid_pairs)} valid image-XML pairs\")\n",
    "        \n",
    "        # If we have more valid pairs than the minimum size, randomly select pairs to move\n",
    "        if len(valid_pairs) > min_size:\n",
    "            excess_count = len(valid_pairs) - min_size\n",
    "            print(f\"    Need to move {excess_count} excess pairs to balance the dataset\")\n",
    "            pairs_to_move = random.sample(valid_pairs, excess_count)\n",
    "            for base, xml_path, jpg_path in pairs_to_move:\n",
    "                try:\n",
    "                    # Move XML file\n",
    "                    xml_dst = os.path.join(removed_croc_dir, f\"{base}.xml\")\n",
    "                    shutil.move(xml_path, xml_dst)\n",
    "                    print(f\"      Moved excess XML: {base}.xml\")\n",
    "                    total_moved_xml += 1\n",
    "                    \n",
    "                    # Move corresponding JPG\n",
    "                    jpg_dst = os.path.join(removed_croc_dir, f\"{base}.jpg\")\n",
    "                    shutil.move(jpg_path, jpg_dst)\n",
    "                    print(f\"      Moved excess image: {base}.jpg\")\n",
    "                    total_moved_jpg += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error moving excess files for {base}: {str(e)}\")\n",
    "        \n",
    "        print(f\"    Folder {croc_id} now has {min_size} valid pairs\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n[STEP 5] Cleaning and balancing complete!\")\n",
    "    total_moved = total_moved_jpg + total_moved_xml\n",
    "    if total_moved == 0:\n",
    "        print(\"\\nNo files needed to be moved. Dataset is clean and balanced!\")\n",
    "    else:\n",
    "        print(f\"\\nMoved {total_moved} files to removed directory:\")\n",
    "        print(f\"- {total_moved_jpg} JPG files\")\n",
    "        print(f\"- {total_moved_xml} XML files\")\n",
    "        print(f\"\\nFiles have been moved to: {removed_dir}\")\n",
    "        print(f\"Each folder now contains {min_size} valid image-XML pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b6cf3",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a476d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize feature extractors with caching\n",
    "        \"\"\"\n",
    "        print(\"[DEBUG] Initializing FeatureExtractor...\")\n",
    "        # Initialize feature extractors\n",
    "        self.sift = cv2.SIFT_create()\n",
    "        self.orb = cv2.ORB_create()\n",
    "        \n",
    "        # Parameters\n",
    "        self.lbp_radius = 3\n",
    "        self.lbp_n_points = 8 * self.lbp_radius\n",
    "        self.hog_orientations = 9\n",
    "        self.hog_pixels_per_cell = (8, 8)\n",
    "        self.hog_cells_per_block = (2, 2)\n",
    "        \n",
    "        # Setup caching in current directory\n",
    "        cache_dir = 'feature_cache'\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"[DEBUG] Cache directory: {cache_dir}\")\n",
    "        self.memory = Memory(cache_dir, verbose=0)\n",
    "        \n",
    "        # Cache the feature extraction methods\n",
    "        print(\"[DEBUG] Setting up feature caching...\")\n",
    "        self.cached_sift = self.memory.cache(self._extract_sift)\n",
    "        self.cached_hog = self.memory.cache(self._extract_hog)\n",
    "        self.cached_lbp = self.memory.cache(self._extract_lbp)\n",
    "        self.cached_orb = self.memory.cache(self._extract_orb)\n",
    "        print(\"[DEBUG] FeatureExtractor initialization complete\")\n",
    "    \n",
    "    def _extract_sift(self, gray):\n",
    "        \"\"\"Internal SIFT feature extraction\"\"\"\n",
    "        keypoints, descriptors = self.sift.detectAndCompute(gray, None)\n",
    "        if descriptors is None:\n",
    "            print(\"[DEBUG] No SIFT features found, returning zero vector\")\n",
    "            return np.zeros(128)\n",
    "        return np.mean(descriptors, axis=0)\n",
    "    \n",
    "    def _extract_hog(self, gray):\n",
    "        \"\"\"Internal HOG feature extraction\"\"\"\n",
    "        return hog(gray, \n",
    "                  orientations=self.hog_orientations,\n",
    "                  pixels_per_cell=self.hog_pixels_per_cell,\n",
    "                  cells_per_block=self.hog_cells_per_block,\n",
    "                  block_norm='L2-Hys')\n",
    "    \n",
    "    def _extract_lbp(self, gray):\n",
    "        \"\"\"Internal LBP feature extraction\"\"\"\n",
    "        lbp = local_binary_pattern(gray, \n",
    "                                 self.lbp_n_points,\n",
    "                                 self.lbp_radius,\n",
    "                                 method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), \n",
    "                             bins=np.arange(0, self.lbp_n_points + 3),\n",
    "                             density=True)\n",
    "        return hist\n",
    "    \n",
    "    def _extract_orb(self, gray):\n",
    "        \"\"\"Internal ORB feature extraction\"\"\"\n",
    "        keypoints, descriptors = self.orb.detectAndCompute(gray, None)\n",
    "        if descriptors is None:\n",
    "            print(\"[DEBUG] No ORB features found, returning zero vector\")\n",
    "            return np.zeros(32)\n",
    "        return np.mean(descriptors, axis=0)\n",
    "    \n",
    "    def extract_all_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract all features from image using parallel processing\n",
    "        \n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Concatenated features\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to grayscale once\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Extract features in parallel\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {\n",
    "                'sift': executor.submit(self.cached_sift, gray),\n",
    "                'hog': executor.submit(self.cached_hog, gray),\n",
    "                'lbp': executor.submit(self.cached_lbp, gray),\n",
    "                'orb': executor.submit(self.cached_orb, gray)\n",
    "            }\n",
    "            \n",
    "            # Get results\n",
    "            features = {name: future.result() for name, future in futures.items()}\n",
    "        \n",
    "        # Concatenate all features\n",
    "        all_features = np.concatenate([\n",
    "            features['sift'],\n",
    "            features['hog'],\n",
    "            features['lbp'],\n",
    "            features['orb']\n",
    "        ])\n",
    "        \n",
    "        return all_features\n",
    "    \n",
    "    def extract_features_batch(self, images, desc=\"Extracting features\"):\n",
    "        \"\"\"\n",
    "        Extract features from a batch of images with progress bar\n",
    "        \n",
    "        Args:\n",
    "            images (list): List of images\n",
    "            desc (str): Description for progress bar\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Array of features\n",
    "        \"\"\"\n",
    "        print(f\"\\n[DEBUG] Starting batch processing of {len(images)} images\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        features = []\n",
    "        for img in tqdm(images, desc=desc, unit=\"img\"):\n",
    "            features.append(self.extract_all_features(img))\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n[DEBUG] Batch processing complete:\")\n",
    "        print(f\"[DEBUG] Total time: {total_time:.2f}s\")\n",
    "        print(f\"[DEBUG] Average time per image: {total_time/len(images):.2f}s\")\n",
    "        print(f\"[DEBUG] Final feature array shape: {np.array(features).shape}\")\n",
    "        \n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016bf8f",
   "metadata": {},
   "source": [
    "## 5. Crocodile Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a835f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CrocodileClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier with memory-efficient components\"\"\"\n",
    "        # Initialize PCA with consistent components and batch size\n",
    "        self.pca = IncrementalPCA(n_components=1500, batch_size=1500)\n",
    "        \n",
    "        # Initialize models with memory-efficient settings\n",
    "        self.models = {\n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                C=10.0,\n",
    "                gamma='scale',\n",
    "                probability=True,\n",
    "                cache_size=500,  # 500MB cache\n",
    "                decision_function_shape='ovr',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        \n",
    "        # Create cache directory\n",
    "        self.cache_dir = 'model_cache'\n",
    "        create_directory(self.cache_dir)\n",
    "    \n",
    "    def preprocess_features(self, features):\n",
    "        \"\"\"Preprocess features using PCA\"\"\"\n",
    "        # Convert to numpy array if not already\n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        features = np.nan_to_num(features, nan=0.0)\n",
    "        \n",
    "        # Apply PCA\n",
    "        reduced_features = self.pca.fit_transform(features)\n",
    "        \n",
    "        # Print memory reduction\n",
    "        print(f\"Memory usage reduced by {features.nbytes / (1024**2):.2f} MB\")\n",
    "        print(f\"Explained variance ratio: {np.sum(self.pca.explained_variance_ratio_):.4f}\")\n",
    "        \n",
    "        return reduced_features\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        # Preprocess features\n",
    "        X_train_processed = self.preprocess_features(X_train)\n",
    "        \n",
    "        # Train each model\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(self.cache_dir, f'{name}_model.joblib')\n",
    "            joblib.dump(model, model_path)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate all models\"\"\"\n",
    "        # Preprocess features\n",
    "        X_test_processed = self.preprocess_features(X_test)\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in self.models.items():\n",
    "            # Load model if not in memory\n",
    "            model_path = os.path.join(self.cache_dir, f'{name}_model.joblib')\n",
    "            if not hasattr(model, 'classes_'):\n",
    "                model = joblib.load(model_path)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} Results:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using all models\"\"\"\n",
    "        # Preprocess features\n",
    "        X_processed = self.preprocess_features(X)\n",
    "        \n",
    "        predictions = {}\n",
    "        for name, model in self.models.items():\n",
    "            # Load model if not in memory\n",
    "            model_path = os.path.join(self.cache_dir, f'{name}_model.joblib')\n",
    "            if not hasattr(model, 'classes_'):\n",
    "                model = joblib.load(model_path)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions[name] = model.predict(X_processed)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2983cb",
   "metadata": {},
   "source": [
    "## 6. Main Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723ab00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CrocodilePipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the crocodile identification pipeline\n",
    "        \"\"\"\n",
    "        # Initialize feature extractor\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        \n",
    "        # Initialize classifier\n",
    "        self.classifier = CrocodileClassifier()\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dirs = {\n",
    "            'training': 'cropped/Training',\n",
    "            'test_known': 'cropped/Test/Known',\n",
    "            'test_unknown': 'cropped/Test/Unknown'\n",
    "        }\n",
    "        for dir_path in self.output_dirs.values():\n",
    "            create_directory(dir_path)\n",
    "    \n",
    "    def process_training_data(self, training_dir):\n",
    "        \"\"\"\n",
    "        Process training data: extract bounding boxes and features\n",
    "        \n",
    "        Args:\n",
    "            training_dir (str): Path to training data directory\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features, labels)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        total_images = 0\n",
    "        processed_folders = 0\n",
    "        \n",
    "        print(\"\\n=== Starting Training Data Processing ===\")\n",
    "        print(f\"Training directory: {training_dir}\")\n",
    "        \n",
    "        # Process each crocodile folder\n",
    "        for croc_dir in os.listdir(training_dir):\n",
    "            croc_path = os.path.join(training_dir, croc_dir)\n",
    "            if not os.path.isdir(croc_path):\n",
    "                continue\n",
    "            \n",
    "            processed_folders += 1\n",
    "            folder_images = 0\n",
    "            \n",
    "            # Check if this folder has already been processed\n",
    "            cropped_dir = os.path.join(self.output_dirs['training'], croc_dir)\n",
    "            if os.path.exists(cropped_dir) and os.path.isdir(cropped_dir):\n",
    "                print(f\"\\n[Folder {processed_folders}] Loading features from processed folder: {croc_dir}\")\n",
    "                # Load features from processed images\n",
    "                for img_file in os.listdir(cropped_dir):\n",
    "                    if not img_file.endswith('.jpg'):\n",
    "                        continue\n",
    "                    \n",
    "                    folder_images += 1\n",
    "                    total_images += 1\n",
    "                    \n",
    "                    # Load cropped image\n",
    "                    img_path = os.path.join(cropped_dir, img_file)\n",
    "                    cropped_img = cv2.imread(img_path)\n",
    "                    \n",
    "                    if cropped_img is None:\n",
    "                        print(f\"  Warning: Could not read image {img_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract features\n",
    "                    try:\n",
    "                        img_features = self.feature_extractor.extract_all_features(cropped_img)\n",
    "                        features.append(img_features)\n",
    "                        labels.append(croc_dir)\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error extracting features from {img_file}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"  Loaded {folder_images} images from {croc_dir}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n[Folder {processed_folders}] Processing new folder: {croc_dir}...\")\n",
    "            \n",
    "            # Process each image in the folder\n",
    "            for img_file in os.listdir(croc_path):\n",
    "                if not img_file.endswith('.jpg'):\n",
    "                    continue\n",
    "                \n",
    "                folder_images += 1\n",
    "                total_images += 1\n",
    "                \n",
    "                # Get image and XML paths\n",
    "                img_path = os.path.join(croc_path, img_file)\n",
    "                xml_path = os.path.join(croc_path, img_file.replace('.jpg', '.xml'))\n",
    "                \n",
    "                try:\n",
    "                    # Parse bounding box\n",
    "                    bbox = parse_voc_xml(xml_path)\n",
    "                    \n",
    "                    # Crop image\n",
    "                    cropped_img = crop_image(img_path, bbox)\n",
    "                    \n",
    "                    # Save cropped image\n",
    "                    output_path = os.path.join(self.output_dirs['training'], croc_dir, img_file)\n",
    "                    create_directory(os.path.dirname(output_path))\n",
    "                    cv2.imwrite(output_path, cropped_img)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    img_features = self.feature_extractor.extract_all_features(cropped_img)\n",
    "                    \n",
    "                    features.append(img_features)\n",
    "                    labels.append(croc_dir)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing {img_file}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  Processed {folder_images} images from {croc_dir}\")\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            raise ValueError(\"No features extracted! Check if the dataset directories are correct.\")\n",
    "        \n",
    "        print(\"\\n=== Training Data Processing Summary ===\")\n",
    "        print(f\"Total folders processed: {processed_folders}\")\n",
    "        print(f\"Total images processed: {total_images}\")\n",
    "        print(f\"Total features extracted: {len(features)}\")\n",
    "        print(f\"Feature dimension: {len(features[0])}\")\n",
    "        print(\"=====================================\\n\")\n",
    "            \n",
    "        return np.array(features), np.array(labels)\n",
    "    \n",
    "    def process_test_data(self, test_dir, is_known=True):\n",
    "        \"\"\"\n",
    "        Process test data: crop images and extract features\n",
    "        \n",
    "        Args:\n",
    "            test_dir (str): Path to test data directory\n",
    "            is_known (bool): Whether the test data is for known crocodiles\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features, labels) if is_known else (features,)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = [] if is_known else None\n",
    "        total_images = 0\n",
    "        \n",
    "        print(f\"\\n=== Processing {'Known' if is_known else 'Unknown'} Test Data ===\")\n",
    "        print(f\"Test directory: {test_dir}\")\n",
    "        \n",
    "        # Process each image\n",
    "        for img_file in os.listdir(test_dir):\n",
    "            if not img_file.endswith('.jpg'):\n",
    "                continue\n",
    "            \n",
    "            total_images += 1\n",
    "            print(f\"\\nProcessing image {total_images}: {img_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Get image path\n",
    "                img_path = os.path.join(test_dir, img_file)\n",
    "                \n",
    "                # Crop image (center crop for unknown)\n",
    "                cropped_img = crop_image(img_path)\n",
    "                \n",
    "                # Save cropped image\n",
    "                output_dir = self.output_dirs['test_known' if is_known else 'test_unknown']\n",
    "                output_path = os.path.join(output_dir, img_file)\n",
    "                cv2.imwrite(output_path, cropped_img)\n",
    "                \n",
    "                # Extract features\n",
    "                img_features = self.feature_extractor.extract_all_features(cropped_img)\n",
    "                \n",
    "                features.append(img_features)\n",
    "                if is_known:\n",
    "                    labels.append(extract_croc_id_from_filename(img_file))\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {img_file}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"\\n=== Test Data Processing Summary ===\")\n",
    "        print(f\"Total images processed: {total_images}\")\n",
    "        print(f\"Total features extracted: {len(features)}\")\n",
    "        if len(features) > 0:\n",
    "            print(f\"Feature dimension: {len(features[0])}\")\n",
    "        print(\"=====================================\\n\")\n",
    "        \n",
    "        if is_known:\n",
    "            return np.array(features), np.array(labels)\n",
    "        return np.array(features)\n",
    "    \n",
    "    def run_pipeline(self, training_dir, test_known_dir, test_unknown_dir):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline\n",
    "        \n",
    "        Args:\n",
    "            training_dir (str): Path to training data directory\n",
    "            test_known_dir (str): Path to known test data directory\n",
    "            test_unknown_dir (str): Path to unknown test data directory\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Starting Crocodile Identification Pipeline ===\")\n",
    "        print(f\"Training directory: {training_dir}\")\n",
    "        print(f\"Known test directory: {test_known_dir}\")\n",
    "        print(f\"Unknown test directory: {test_unknown_dir}\")\n",
    "        print(\"=============================================\\n\")\n",
    "        \n",
    "        # Clean and balance the dataset\n",
    "        print(\"Cleaning and balancing dataset...\")\n",
    "        clean_and_balance_dataset(training_dir)\n",
    "        \n",
    "        # Process training data\n",
    "        print(\"\\nProcessing training data...\")\n",
    "        X_train, y_train = self.process_training_data(training_dir)\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        print(\"\\nTraining and evaluating models...\")\n",
    "        self.classifier.train(X_train, y_train)\n",
    "        \n",
    "        # Process and evaluate test data\n",
    "        print(\"\\nProcessing known test data...\")\n",
    "        X_test_known, y_test_known = self.process_test_data(test_known_dir, is_known=True)\n",
    "        known_results = self.classifier.evaluate(X_test_known, y_test_known)\n",
    "        \n",
    "        print(\"\\nProcessing unknown test data...\")\n",
    "        X_test_unknown = self.process_test_data(test_unknown_dir, is_known=False)\n",
    "        unknown_predictions = self.classifier.predict(X_test_unknown)\n",
    "        \n",
    "        print(\"\\n=== Pipeline Completed Successfully ===\")\n",
    "        \n",
    "        return known_results, unknown_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470e78d",
   "metadata": {},
   "source": [
    "## 7. Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7749372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = CrocodilePipeline()\n",
    "\n",
    "# Run pipeline with dataset directories\n",
    "known_results, unknown_predictions = pipeline.run_pipeline(\n",
    "    training_dir=\"dataset/Training\",\n",
    "    test_known_dir=\"dataset/Test/Known\",\n",
    "    test_unknown_dir=\"dataset/Test/Unknown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b21e4",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The pipeline has completed successfully! Here's what was accomplished:\")\n",
    "\n",
    "print(\"\\n1. Dataset Preparation:\")\n",
    "print(\"   - Cleaned and balanced the training dataset\")\n",
    "print(\"   - Removed invalid and orphaned files\")\n",
    "print(\"   - Ensured equal number of samples per class\")\n",
    "\n",
    "print(\"\\n2. Feature Extraction:\")\n",
    "print(\"   - Extracted SIFT, HOG, LBP, and ORB features\")\n",
    "print(\"   - Implemented parallel processing for efficiency\")\n",
    "print(\"   - Cached features to speed up processing\")\n",
    "\n",
    "print(\"\\n3. Model Training:\")\n",
    "print(\"   - Trained SVM, Random Forest, and XGBoost models\")\n",
    "print(\"   - Applied PCA for dimensionality reduction\")\n",
    "print(\"   - Optimized memory usage\")\n",
    "\n",
    "print(\"\\n4. Evaluation:\")\n",
    "print(\"   - Evaluated models on known test data\")\n",
    "print(\"   - Generated predictions for unknown test data\")\n",
    "print(\"   - Calculated performance metrics\")\n",
    "\n",
    "print(\"\\nThe results are stored in `known_results` and `unknown_predictions` variables. You can analyze these results further or use the trained models for new predictions.\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
